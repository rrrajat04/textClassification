{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f364403a-3515-4b53-9419-b8366501fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It doesn't make much sense to compute the validation metrics at each iteration, because it would make your training process much slower and your model doesn't change that much from iteration to iteration\n",
      "\n",
      ". On the other hand it makes much more sense to compute these metrics at the end of each epoch\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "In your case you have 50000 samples on the training set and 10000 samples on the validation set and a batch size of 32\n",
      "\n",
      ". If you were to compute the val_loss and val_acc after each iteration it would mean that for every 32 training samples updating your weights you would have 313 (i.e. 10000/32) iterations of 32 validation samples\n",
      "\n",
      ". Since your every epoch consists of 1563 iterations (i.e. 50000/32), you'd have to perform 489219 (i.e. 313*1563) batch predictions just for evaluating the model\n",
      "\n",
      ". This would cause your model to train several orders of magnitude slower\n",
      "\n",
      "!\n",
      "\n",
      "If you still want to compute the validation metrics at the end of each iteration (not recommended for the reasons stated above), you could simply shorten your \"epoch\" so that your model sees\n",
      "\n",
      "just\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the function for paragraph-based chunking\n",
    "def chunk_paragraph(text, chunk_size):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i, token in enumerate(doc):\n",
    "        if i == start + chunk_size or token.is_sent_end:\n",
    "            chunks.append(doc[start:i])\n",
    "            start = i\n",
    "    if start < len(doc):\n",
    "        chunks.append(doc[start:])\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"It doesn't make much sense to compute the validation metrics at each iteration, because it would make your training process much slower and your model doesn't change that much from iteration to iteration. On the other hand it makes much more sense to compute these metrics at the end of each epoch.\n",
    "\n",
    "In your case you have 50000 samples on the training set and 10000 samples on the validation set and a batch size of 32. If you were to compute the val_loss and val_acc after each iteration it would mean that for every 32 training samples updating your weights you would have 313 (i.e. 10000/32) iterations of 32 validation samples. Since your every epoch consists of 1563 iterations (i.e. 50000/32), you'd have to perform 489219 (i.e. 313*1563) batch predictions just for evaluating the model. This would cause your model to train several orders of magnitude slower!\n",
    "\n",
    "If you still want to compute the validation metrics at the end of each iteration (not recommended for the reasons stated above), you could simply shorten your \"epoch\" so that your model sees just\"\"\"\n",
    "chunks = chunk_paragraph(text, 100)\n",
    "for chunk in chunks:\n",
    "    print(chunk.text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec975e9-09ef-4120-87ff-e5eb573d354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\rrraj\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\rrraj\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\rrraj\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# download the pre-trained model\n",
    "#stanfordnlp.download('en')\n",
    "\n",
    "# load the model\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos', lang='en')\n",
    "\n",
    "def chunk_paragraphs(text):\n",
    "    nlp = stanfordnlp.Pipeline(processors='tokenize')\n",
    "    doc = nlp(text)\n",
    "    paragraphs = [list(sentences.words) for sentences in doc.sentences]\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        chunk = []\n",
    "        for word in para:\n",
    "            if len(' '.join(chunk + [word.text])) > 100:\n",
    "                chunks.append(' '.join(chunk))\n",
    "                chunk = [word.text]\n",
    "            else:\n",
    "                chunk.append(word.text)\n",
    "        chunks.append(' '.join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8aa93d2-a06d-4f4e-afb2-960dff68f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\rrraj\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "st = chunk_paragraphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9135404-9473-4b6b-93e8-78dc936c9f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iteration .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a72a93-e97c-4d5a-9dbf-48a1d193411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordnlp\n",
      "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
      "     -------------------------------------- 158.8/158.8 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from stanfordnlp) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from stanfordnlp) (2.28.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from stanfordnlp) (4.22.1)\n",
      "Collecting torch>=1.0.0\n",
      "  Downloading torch-2.0.0-cp38-cp38-win_amd64.whl (172.3 MB)\n",
      "     -------------------------------------- 172.3/172.3 MB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from stanfordnlp) (1.23.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (3.1.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 12.0 MB/s eta 0:00:00\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "     ---------------------------------------- 6.5/6.5 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from requests->stanfordnlp) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from requests->stanfordnlp) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from requests->stanfordnlp) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from requests->stanfordnlp) (1.26.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from tqdm->stanfordnlp) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rrraj\\anaconda3\\envs\\mlops\\lib\\site-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, filelock, torch, stanfordnlp\n",
      "Successfully installed filelock-3.11.0 mpmath-1.3.0 networkx-3.1 stanfordnlp-0.2.0 sympy-1.11.1 torch-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c98fc-84e7-495a-8312-5935e53209a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
